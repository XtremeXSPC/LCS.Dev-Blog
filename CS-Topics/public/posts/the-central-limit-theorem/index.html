<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Central Limit Theorem | LCS.Developer</title>
<meta name=keywords content="Probability,Math"><meta name=description content="A comprehensive explanation of the Central Limit Theorem with proofs and applications."><meta name=author content="LCS.Dev"><link rel=canonical href=https://lcs-developer.com/posts/the-central-limit-theorem/><meta name=google-site-verification content="CPQPKX3QPR"><link crossorigin=anonymous href=/assets/css/stylesheet.5f2c0c3e23aa8b25a8880b02378cbabfa0a60e696ce1dd8b60ba968d02b64a2f.css integrity="sha256-XywMPiOqiyWoiAsCN4y6v6CmDmls4d2LYLqWjQK2Si8=" rel="preload stylesheet" as=style><link rel=icon href=https://lcs-developer.com/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lcs-developer.com/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lcs-developer.com/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://lcs-developer.com/favicon/apple-touch-icon.png><link rel=mask-icon href=https://lcs-developer.com/favicon/safari-pinned-tab.svg><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#ffffff"><link rel=alternate hreflang=en href=https://lcs-developer.com/posts/the-central-limit-theorem/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css integrity=sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js integrity=sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!0})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-CPQPKX3QPR"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-CPQPKX3QPR")}</script><meta property="og:url" content="https://lcs-developer.com/posts/the-central-limit-theorem/"><meta property="og:site_name" content="LCS.Developer"><meta property="og:title" content="The Central Limit Theorem"><meta property="og:description" content="A comprehensive explanation of the Central Limit Theorem with proofs and applications."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-23T23:11:30+00:00"><meta property="article:modified_time" content="2024-12-23T23:11:30+00:00"><meta property="article:tag" content="Probability"><meta property="article:tag" content="Math"><meta property="og:image" content="https://lcs-developer.com/images/LCS.Dev-Logo.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://lcs-developer.com/images/LCS.Dev-Logo.jpg"><meta name=twitter:title content="The Central Limit Theorem"><meta name=twitter:description content="A comprehensive explanation of the Central Limit Theorem with proofs and applications."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lcs-developer.com/posts/"},{"@type":"ListItem","position":2,"name":"The Central Limit Theorem","item":"https://lcs-developer.com/posts/the-central-limit-theorem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Central Limit Theorem","name":"The Central Limit Theorem","description":"A comprehensive explanation of the Central Limit Theorem with proofs and applications.","keywords":["Probability","Math"],"articleBody":"Introduction The Central Limit Theorem (CLT) is a fundamental result in probability theory and statistics that has profound implications for data analysis, hypothesis testing, and statistical inference. First formulated in the 18th century and rigorously proven in the early 20th century, the CLT establishes the surprising fact that the sum (or average) of a large number of independent, identically distributed random variables tends toward a normal distribution, regardless of the shape of the original distribution.\nThis remarkable property explains why the normal (or Gaussian) distribution appears so frequently in natural phenomena and why it forms the backbone of many statistical methods. When we observe variables that represent the sum of many small, independent influences—such as measurement errors, biological variations, or economic indicators—the CLT provides the theoretical justification for why these variables often follow a bell-shaped curve.\nHistorical Context The development of the Central Limit Theorem spans several centuries:\nAbraham de Moivre (1733) first discovered that the binomial distribution could be approximated by a continuous curve (now known as the normal distribution) as the number of trials increases. Pierre-Simon Laplace (1812) extended this work and formulated an early version of the theorem. Siméon Denis Poisson contributed further to the understanding of the theorem in the early 19th century. Aleksandr Lyapunov (1901) provided the first rigorous mathematical proof using characteristic functions. Jarl Waldemar Lindeberg and Paul Lévy further refined the conditions and proof in the 1920s. Formal Statement of the Theorem Let $X_1, X_2, \\ldots, X_n$ be independent and identically distributed (i.i.d.) random variables with:\nExpected value (mean): $\\mu = \\mathbb{E}[X_i]$ Variance: $\\sigma^2 = \\text{Var}(X_i) \u003c \\infty$ Define the sum: $$ S_n = \\sum_{i=1}^n X_i $$\nThe normalized (or standardized) sum is: $$ Z_n = \\frac{S_n - n\\mu}{\\sigma\\sqrt{n}} $$\nThe Central Limit Theorem states that: $$ Z_n \\xrightarrow{d} \\mathcal{N}(0, 1) \\quad \\text{as } n \\to \\infty $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution, and $\\mathcal{N}(0, 1)$ represents the standard normal distribution.\nEquivalently, for large $n$: $$ S_n \\approx \\mathcal{N}(n\\mu, n\\sigma^2) $$\nThis means that the sum $S_n$ is approximately normally distributed with mean $n\\mu$ and variance $n\\sigma^2$ when $n$ is sufficiently large.\nIntuitive Explanation and Examples Why Does This Happen? The CLT occurs because when we add many independent random variables, their individual peculiarities tend to average out. The “peaks and valleys” of the original distribution smooth into a bell-shaped curve as more variables are added.\nThis can be understood through three key insights:\nAveraging effect: Random variables above the mean tend to be balanced by those below the mean, creating a central tendency. Variance accumulation: The variance of the sum increases linearly with the number of terms, but the standard deviation increases only with the square root of $n$. Combinatorial explosion: There are far more ways to achieve values near the mean than extreme values when combining many random variables. Concrete Example: Dice Rolling Consider rolling a single six-sided die. The probability distribution is uniform—each outcome (1 through 6) has equal probability (1/6). This distribution is clearly not normal.\nNow consider the sum of rolling two dice. The possible outcomes range from 2 to 12, with a triangular probability distribution peaking at 7.\nAs we increase the number of dice, the distribution of the sum becomes increasingly bell-shaped:\nWith 3 dice: The distribution begins to show a more rounded peak With 10 dice: The distribution closely resembles a normal distribution With 100 dice: The distribution is practically indistinguishable from a normal distribution This progression illustrates the CLT in action. Even though each individual die has a uniform distribution, the sum of many dice approaches a normal distribution.\nExample: Sampling from a Bernoulli Distribution Consider a biased coin with probability $p = 0.7$ of heads. If we flip this coin $n$ times and count the number of heads, this follows a binomial distribution with parameters $n$ and $p$.\nFor large $n$, the binomial distribution can be approximated by: $$ \\text{Binomial}(n, p) \\approx \\mathcal{N}(np, np(1-p)) $$\nThis is a direct application of the CLT, since a binomial random variable is the sum of $n$ independent Bernoulli random variables.\nRigorous Proof Using Characteristic Functions While there are several approaches to proving the CLT, the method using characteristic functions is particularly elegant and powerful. A characteristic function uniquely determines a probability distribution and simplifies the analysis of sums of independent random variables.\nDefinition of Characteristic Function The characteristic function of a random variable $X$ is defined as: $$ \\phi_X(t) = \\mathbb{E}[e^{itX}] = \\int_{-\\infty}^{\\infty} e^{itx} f_X(x) , dx $$\nwhere $i = \\sqrt{-1}$, $t$ is a real number, and $f_X(x)$ is the probability density function of $X$.\nKey Properties of Characteristic Functions Sum of independent random variables: If $X$ and $Y$ are independent, then: $$\\phi_{X+Y}(t) = \\phi_X(t) \\cdot \\phi_Y(t)$$\nLinear transformation: For constants $a$ and $b$: $$\\phi_{aX+b}(t) = e^{itb} \\cdot \\phi_X(at)$$\nStandard normal distribution: The characteristic function of $\\mathcal{N}(0, 1)$ is: $$\\phi_Z(t) = e^{-t^2/2}$$\nProof Sketch Taylor expansion: For any random variable $X$ with mean $\\mu$ and variance $\\sigma^2$, the Taylor expansion of its characteristic function around $t = 0$ gives: $$\\phi_X(t) = 1 + it\\mu - \\frac{\\sigma^2 t^2}{2} + o(t^2)$$\nCharacteristic function of the sum: For i.i.d. random variables $X_1, X_2, \\ldots, X_n$, the characteristic function of their sum is: $$\\phi_{S_n}(t) = [\\phi_X(t)]^n$$\nNormalized sum: The characteristic function of the standardized sum $Z_n$ is: $$\\phi_{Z_n}(t) = \\phi_{(S_n - n\\mu)/(\\sigma\\sqrt{n})}(t) = e^{-it\\frac{n\\mu}{\\sigma\\sqrt{n}}} \\cdot \\phi_{S_n}\\left(\\frac{t}{\\sigma\\sqrt{n}}\\right)$$\nSubstitution and expansion: $$\\phi_{Z_n}(t) = e^{-it\\frac{n\\mu}{\\sigma\\sqrt{n}}} \\cdot \\left[\\phi_X\\left(\\frac{t}{\\sigma\\sqrt{n}}\\right)\\right]^n$$\n$$= e^{-it\\frac{n\\mu}{\\sigma\\sqrt{n}}} \\cdot \\left[1 + it\\frac{\\mu}{\\sigma\\sqrt{n}} - \\frac{\\sigma^2 t^2}{2\\sigma^2 n} + o\\left(\\frac{1}{n}\\right)\\right]^n$$\n$$= e^{-it\\frac{n\\mu}{\\sigma\\sqrt{n}}} \\cdot \\left[1 + it\\frac{\\mu}{\\sigma\\sqrt{n}} - \\frac{t^2}{2n} + o\\left(\\frac{1}{n}\\right)\\right]^n$$\nTaking the limit: As $n \\to \\infty$, using the limit formula $(1 + \\frac{x}{n})^n \\to e^x$: $$\\lim_{n \\to \\infty} \\phi_{Z_n}(t) = e^{-it\\frac{n\\mu}{\\sigma\\sqrt{n}}} \\cdot e^{it\\frac{n\\mu}{\\sigma\\sqrt{n}} - \\frac{t^2}{2}} = e^{-\\frac{t^2}{2}}$$\nConclusion: Since $e^{-\\frac{t^2}{2}}$ is the characteristic function of the standard normal distribution $\\mathcal{N}(0, 1)$, and characteristic functions uniquely determine distributions, we conclude that $Z_n$ converges in distribution to $\\mathcal{N}(0, 1)$.\nLindeberg-Lévy Condition For the above proof to be valid, we need the Lindeberg-Lévy condition, which essentially requires that no single random variable dominates the sum. For i.i.d. random variables with finite variance, this condition is automatically satisfied.\nSample Means and the CLT A particularly important application of the CLT relates to sample means. Let’s define the sample mean: $$ \\bar{X}n = \\frac{1}{n}\\sum{i=1}^n X_i $$\nThe CLT can be restated in terms of the sample mean: $$ \\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1) \\quad \\text{as } n \\to \\infty $$\nEquivalently: $$ \\bar{X}_n \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\quad \\text{for large } n $$\nThis formulation of the CLT has profound implications for statistical estimation and inference:\nConsistency: As sample size increases, the sample mean converges to the true population mean. Efficiency: The standard error of the mean decreases at a rate of $1/\\sqrt{n}$. Normality: Regardless of the original distribution, the sampling distribution of the mean approaches normality. Assumptions and Violations Key Assumptions The classical CLT relies on several assumptions:\nIndependence: The random variables must be independent of each other. Identical distribution: The random variables must come from the same distribution. Finite variance: The variance $\\sigma^2$ must be finite. Extensions and Generalizations Several generalizations of the CLT exist for cases where the classical assumptions are not met:\nLyapunov CLT: Relaxes the identical distribution requirement, requiring only that no single variable dominates the sum. Lindeberg-Feller CLT: Provides more general conditions for the convergence of non-identical random variables. Martingale CLT: Extends to certain dependent sequences of random variables. Stable distributions: When the variance is infinite, sums may converge to non-normal stable distributions. When the CLT Fails The CLT may not apply or may require larger sample sizes in several scenarios:\nHeavy-tailed distributions: For distributions with infinite variance (e.g., Cauchy distribution), the CLT does not apply in its standard form. Strong dependencies: When variables are strongly correlated, the independence assumption is violated. Mixture distributions: For multimodal distributions, larger sample sizes may be needed for the CLT approximation to be accurate. Discrete distributions with few possible values: The convergence to normality may be slower. Applications in Statistics The CLT underpins many statistical methods and applications:\nHypothesis Testing Statistical tests like the t-test, z-test, and ANOVA rely on the CLT for their validity when sample sizes are sufficiently large.\nConfidence Intervals For large samples, we can construct confidence intervals for the population mean: $$ \\bar{X} \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}} $$\nwhere $z_{\\alpha/2}$ is the critical value from the standard normal distribution.\nQuality Control In manufacturing, the CLT justifies the use of control charts to monitor process means and detect deviations from target specifications.\nSurvey Sampling The CLT provides the foundation for calculating margins of error in opinion polls and survey research.\nBootstrap Methods The bootstrap technique relies on the CLT to approximate sampling distributions of statistics when analytical forms are unavailable.\nRelated Theorems Law of Large Numbers (LLN) The LLN states that the sample mean converges to the expected value as the sample size increases: $$ \\bar{X}_n \\xrightarrow{p} \\mu \\quad \\text{as } n \\to \\infty $$\nwhere $\\xrightarrow{p}$ denotes convergence in probability.\nWhile the LLN tells us that the sample mean approaches the population mean, the CLT provides information about the distribution of that approximation and how quickly it converges.\nBerry-Esseen Theorem The Berry-Esseen theorem quantifies the rate of convergence to the normal distribution. For i.i.d. random variables with finite third moment $\\rho = \\mathbb{E}[|X-\\mu|^3]$, the theorem provides an upper bound on the approximation error: $$ \\sup_{x \\in \\mathbb{R}} |F_{Z_n}(x) - \\Phi(x)| \\leq \\frac{C\\rho}{\\sigma^3\\sqrt{n}} $$\nwhere $F_{Z_n}$ is the CDF of $Z_n$, $\\Phi$ is the standard normal CDF, and $C$ is a constant.\nMultivariate Central Limit Theorem The multivariate CLT extends the result to vector-valued random variables, stating that the normalized sum of i.i.d. random vectors converges to a multivariate normal distribution.\nConclusion The Central Limit Theorem stands as one of the most profound results in probability theory, serving as a bridge between theoretical mathematics and practical statistics. Its assertion that sums of random variables tend toward normality, regardless of their original distribution, provides the foundation for numerous statistical methods and applications.\nThe universality of the CLT helps explain why the normal distribution appears so frequently in nature and human affairs—many observed phenomena represent the cumulative effect of numerous small, independent factors. From measurement errors to biological traits, financial returns to production quality, the CLT offers a mathematical explanation for the prevalence of bell-shaped distributions.\nUnderstanding the CLT, including its assumptions, limitations, and generalizations, is essential for anyone working with data and statistical analysis. While the formal proof may involve sophisticated mathematics, the intuitive concept—that aggregating many random influences tends to produce a normal distribution—remains accessible and profoundly useful across disciplines.\nAs we collect larger datasets and develop more sophisticated analytical techniques, the Central Limit Theorem continues to serve as a cornerstone of statistical inference, allowing us to make reliable statements about populations based on samples, even when the underlying distributions are unknown or complex.\nReferences DasGupta, A. (2008). Asymptotic Theory of Statistics and Probability. Springer. Feller, W. (1971). An Introduction to Probability Theory and Its Applications, Vol. 2. Wiley. Gnedenko, B.V., \u0026 Kolmogorov, A.N. (1968). Limit Distributions for Sums of Independent Random Variables. Addison-Wesley. Le Cam, L. (1986). Asymptotic Methods in Statistical Decision Theory. Springer. Lehmann, E.L. (1999). Elements of Large-Sample Theory. Springer. ","wordCount":"1862","inLanguage":"en","image":"https://lcs-developer.com/images/LCS.Dev-Logo.jpg","datePublished":"2024-12-23T23:11:30.67541Z","dateModified":"2024-12-23T23:11:30.67541Z","author":{"@type":"Person","name":"LCS.Dev"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://lcs-developer.com/posts/the-central-limit-theorem/"},"publisher":{"@type":"Organization","name":"LCS.Developer","logo":{"@type":"ImageObject","url":"https://lcs-developer.com/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lcs-developer.com/ accesskey=h title="LCS.Dev - CS Topics (Alt + H)"><img src=https://lcs-developer.com/%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB alt aria-label=logo height=35>LCS.Dev - CS Topics</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lcs-developer.com/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://lcs-developer.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://lcs-developer.com/about/ title=About><span>About</span></a></li><li><a href=https://lcs-developer.com/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://lcs-developer.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://lcs-developer.com/>Home</a>&nbsp;»&nbsp;<a href=https://lcs-developer.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">The Central Limit Theorem</h1><div class=post-description>A comprehensive explanation of the Central Limit Theorem with proofs and applications.</div><div class=post-meta><span title='2024-12-23 23:11:30.67541 +0000 UTC'>December 23, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1862 words&nbsp;·&nbsp;LCS.Dev&nbsp;|&nbsp;<a href=https://github.com/XtremeXSPC/LCS.Dev-Blog/tree/hostinger//posts/The%20Central%20Limit%20Theorem.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#historical-context>Historical Context</a></li></ul></li><li><a href=#formal-statement-of-the-theorem>Formal Statement of the Theorem</a></li><li><a href=#intuitive-explanation-and-examples>Intuitive Explanation and Examples</a><ul><li><a href=#why-does-this-happen>Why Does This Happen?</a></li><li><a href=#concrete-example-dice-rolling>Concrete Example: Dice Rolling</a></li><li><a href=#example-sampling-from-a-bernoulli-distribution>Example: Sampling from a Bernoulli Distribution</a></li></ul></li><li><a href=#rigorous-proof-using-characteristic-functions>Rigorous Proof Using Characteristic Functions</a><ul><li><a href=#definition-of-characteristic-function>Definition of Characteristic Function</a></li><li><a href=#key-properties-of-characteristic-functions>Key Properties of Characteristic Functions</a></li><li><a href=#proof-sketch>Proof Sketch</a></li><li><a href=#lindeberg-lévy-condition>Lindeberg-Lévy Condition</a></li></ul></li><li><a href=#sample-means-and-the-clt>Sample Means and the CLT</a></li><li><a href=#assumptions-and-violations>Assumptions and Violations</a><ul><li><a href=#key-assumptions>Key Assumptions</a></li><li><a href=#extensions-and-generalizations>Extensions and Generalizations</a></li><li><a href=#when-the-clt-fails>When the CLT Fails</a></li></ul></li><li><a href=#applications-in-statistics>Applications in Statistics</a><ul><li><a href=#hypothesis-testing>Hypothesis Testing</a></li><li><a href=#confidence-intervals>Confidence Intervals</a></li><li><a href=#quality-control>Quality Control</a></li><li><a href=#survey-sampling>Survey Sampling</a></li><li><a href=#bootstrap-methods>Bootstrap Methods</a></li></ul></li><li><a href=#related-theorems>Related Theorems</a><ul><li><a href=#law-of-large-numbers-lln>Law of Large Numbers (LLN)</a></li><li><a href=#berry-esseen-theorem>Berry-Esseen Theorem</a></li><li><a href=#multivariate-central-limit-theorem>Multivariate Central Limit Theorem</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>The <strong>Central Limit Theorem (CLT)</strong> is a fundamental result in probability theory and statistics that has profound implications for data analysis, hypothesis testing, and statistical inference. First formulated in the 18th century and rigorously proven in the early 20th century, the CLT establishes the surprising fact that the sum (or average) of a large number of independent, identically distributed random variables tends toward a normal distribution, regardless of the shape of the original distribution.</p><p>This remarkable property explains why the normal (or Gaussian) distribution appears so frequently in natural phenomena and why it forms the backbone of many statistical methods. When we observe variables that represent the sum of many small, independent influences—such as measurement errors, biological variations, or economic indicators—the CLT provides the theoretical justification for why these variables often follow a bell-shaped curve.</p><h3 id=historical-context>Historical Context<a hidden class=anchor aria-hidden=true href=#historical-context>#</a></h3><p>The development of the Central Limit Theorem spans several centuries:</p><ul><li><strong>Abraham de Moivre</strong> (1733) first discovered that the binomial distribution could be approximated by a continuous curve (now known as the normal distribution) as the number of trials increases.</li><li><strong>Pierre-Simon Laplace</strong> (1812) extended this work and formulated an early version of the theorem.</li><li><strong>Siméon Denis Poisson</strong> contributed further to the understanding of the theorem in the early 19th century.</li><li><strong>Aleksandr Lyapunov</strong> (1901) provided the first rigorous mathematical proof using characteristic functions.</li><li><strong>Jarl Waldemar Lindeberg</strong> and <strong>Paul Lévy</strong> further refined the conditions and proof in the 1920s.</li></ul><h2 id=formal-statement-of-the-theorem>Formal Statement of the Theorem<a hidden class=anchor aria-hidden=true href=#formal-statement-of-the-theorem>#</a></h2><p>Let $X_1, X_2, \ldots, X_n$ be independent and identically distributed (i.i.d.) random variables with:</p><ul><li>Expected value (mean): $\mu = \mathbb{E}[X_i]$</li><li>Variance: $\sigma^2 = \text{Var}(X_i) &lt; \infty$</li></ul><p>Define the sum: $$ S_n = \sum_{i=1}^n X_i $$</p><p>The normalized (or standardized) sum is: $$ Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}} $$</p><p>The Central Limit Theorem states that: $$ Z_n \xrightarrow{d} \mathcal{N}(0, 1) \quad \text{as } n \to \infty $$</p><p>where $\xrightarrow{d}$ denotes convergence in distribution, and $\mathcal{N}(0, 1)$ represents the standard normal distribution.</p><p>Equivalently, for large $n$: $$ S_n \approx \mathcal{N}(n\mu, n\sigma^2) $$</p><p>This means that the sum $S_n$ is approximately normally distributed with mean $n\mu$ and variance $n\sigma^2$ when $n$ is sufficiently large.</p><h2 id=intuitive-explanation-and-examples>Intuitive Explanation and Examples<a hidden class=anchor aria-hidden=true href=#intuitive-explanation-and-examples>#</a></h2><h3 id=why-does-this-happen>Why Does This Happen?<a hidden class=anchor aria-hidden=true href=#why-does-this-happen>#</a></h3><p>The CLT occurs because when we add many independent random variables, their individual peculiarities tend to average out. The &ldquo;peaks and valleys&rdquo; of the original distribution smooth into a bell-shaped curve as more variables are added.</p><p>This can be understood through three key insights:</p><ol><li><strong>Averaging effect</strong>: Random variables above the mean tend to be balanced by those below the mean, creating a central tendency.</li><li><strong>Variance accumulation</strong>: The variance of the sum increases linearly with the number of terms, but the standard deviation increases only with the square root of $n$.</li><li><strong>Combinatorial explosion</strong>: There are far more ways to achieve values near the mean than extreme values when combining many random variables.</li></ol><h3 id=concrete-example-dice-rolling>Concrete Example: Dice Rolling<a hidden class=anchor aria-hidden=true href=#concrete-example-dice-rolling>#</a></h3><p>Consider rolling a single six-sided die. The probability distribution is uniform—each outcome (1 through 6) has equal probability (1/6). This distribution is clearly not normal.</p><p>Now consider the sum of rolling two dice. The possible outcomes range from 2 to 12, with a triangular probability distribution peaking at 7.</p><p>As we increase the number of dice, the distribution of the sum becomes increasingly bell-shaped:</p><ul><li>With 3 dice: The distribution begins to show a more rounded peak</li><li>With 10 dice: The distribution closely resembles a normal distribution</li><li>With 100 dice: The distribution is practically indistinguishable from a normal distribution</li></ul><p>This progression illustrates the CLT in action. Even though each individual die has a uniform distribution, the sum of many dice approaches a normal distribution.</p><h3 id=example-sampling-from-a-bernoulli-distribution>Example: Sampling from a Bernoulli Distribution<a hidden class=anchor aria-hidden=true href=#example-sampling-from-a-bernoulli-distribution>#</a></h3><p>Consider a biased coin with probability $p = 0.7$ of heads. If we flip this coin $n$ times and count the number of heads, this follows a binomial distribution with parameters $n$ and $p$.</p><p>For large $n$, the binomial distribution can be approximated by: $$ \text{Binomial}(n, p) \approx \mathcal{N}(np, np(1-p)) $$</p><p>This is a direct application of the CLT, since a binomial random variable is the sum of $n$ independent Bernoulli random variables.</p><h2 id=rigorous-proof-using-characteristic-functions>Rigorous Proof Using Characteristic Functions<a hidden class=anchor aria-hidden=true href=#rigorous-proof-using-characteristic-functions>#</a></h2><p>While there are several approaches to proving the CLT, the method using characteristic functions is particularly elegant and powerful. A characteristic function uniquely determines a probability distribution and simplifies the analysis of sums of independent random variables.</p><h3 id=definition-of-characteristic-function>Definition of Characteristic Function<a hidden class=anchor aria-hidden=true href=#definition-of-characteristic-function>#</a></h3><p>The characteristic function of a random variable $X$ is defined as: $$ \phi_X(t) = \mathbb{E}[e^{itX}] = \int_{-\infty}^{\infty} e^{itx} f_X(x) , dx $$</p><p>where $i = \sqrt{-1}$, $t$ is a real number, and $f_X(x)$ is the probability density function of $X$.</p><h3 id=key-properties-of-characteristic-functions>Key Properties of Characteristic Functions<a hidden class=anchor aria-hidden=true href=#key-properties-of-characteristic-functions>#</a></h3><ol><li><p><strong>Sum of independent random variables</strong>: If $X$ and $Y$ are independent, then: $$\phi_{X+Y}(t) = \phi_X(t) \cdot \phi_Y(t)$$</p></li><li><p><strong>Linear transformation</strong>: For constants $a$ and $b$: $$\phi_{aX+b}(t) = e^{itb} \cdot \phi_X(at)$$</p></li><li><p><strong>Standard normal distribution</strong>: The characteristic function of $\mathcal{N}(0, 1)$ is: $$\phi_Z(t) = e^{-t^2/2}$$</p></li></ol><h3 id=proof-sketch>Proof Sketch<a hidden class=anchor aria-hidden=true href=#proof-sketch>#</a></h3><ol><li><p><strong>Taylor expansion</strong>: For any random variable $X$ with mean $\mu$ and variance $\sigma^2$, the Taylor expansion of its characteristic function around $t = 0$ gives: $$\phi_X(t) = 1 + it\mu - \frac{\sigma^2 t^2}{2} + o(t^2)$$</p></li><li><p><strong>Characteristic function of the sum</strong>: For i.i.d. random variables $X_1, X_2, \ldots, X_n$, the characteristic function of their sum is: $$\phi_{S_n}(t) = [\phi_X(t)]^n$$</p></li><li><p><strong>Normalized sum</strong>: The characteristic function of the standardized sum $Z_n$ is: $$\phi_{Z_n}(t) = \phi_{(S_n - n\mu)/(\sigma\sqrt{n})}(t) = e^{-it\frac{n\mu}{\sigma\sqrt{n}}} \cdot \phi_{S_n}\left(\frac{t}{\sigma\sqrt{n}}\right)$$</p></li><li><p><strong>Substitution and expansion</strong>: $$\phi_{Z_n}(t) = e^{-it\frac{n\mu}{\sigma\sqrt{n}}} \cdot \left[\phi_X\left(\frac{t}{\sigma\sqrt{n}}\right)\right]^n$$</p><p>$$= e^{-it\frac{n\mu}{\sigma\sqrt{n}}} \cdot \left[1 + it\frac{\mu}{\sigma\sqrt{n}} - \frac{\sigma^2 t^2}{2\sigma^2 n} + o\left(\frac{1}{n}\right)\right]^n$$</p><p>$$= e^{-it\frac{n\mu}{\sigma\sqrt{n}}} \cdot \left[1 + it\frac{\mu}{\sigma\sqrt{n}} - \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right]^n$$</p></li><li><p><strong>Taking the limit</strong>: As $n \to \infty$, using the limit formula $(1 + \frac{x}{n})^n \to e^x$: $$\lim_{n \to \infty} \phi_{Z_n}(t) = e^{-it\frac{n\mu}{\sigma\sqrt{n}}} \cdot e^{it\frac{n\mu}{\sigma\sqrt{n}} - \frac{t^2}{2}} = e^{-\frac{t^2}{2}}$$</p></li><li><p><strong>Conclusion</strong>: Since $e^{-\frac{t^2}{2}}$ is the characteristic function of the standard normal distribution $\mathcal{N}(0, 1)$, and characteristic functions uniquely determine distributions, we conclude that $Z_n$ converges in distribution to $\mathcal{N}(0, 1)$.</p></li></ol><h3 id=lindeberg-lévy-condition>Lindeberg-Lévy Condition<a hidden class=anchor aria-hidden=true href=#lindeberg-lévy-condition>#</a></h3><p>For the above proof to be valid, we need the Lindeberg-Lévy condition, which essentially requires that no single random variable dominates the sum. For i.i.d. random variables with finite variance, this condition is automatically satisfied.</p><h2 id=sample-means-and-the-clt>Sample Means and the CLT<a hidden class=anchor aria-hidden=true href=#sample-means-and-the-clt>#</a></h2><p>A particularly important application of the CLT relates to sample means. Let&rsquo;s define the sample mean: $$ \bar{X}<em>n = \frac{1}{n}\sum</em>{i=1}^n X_i $$</p><p>The CLT can be restated in terms of the sample mean: $$ \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1) \quad \text{as } n \to \infty $$</p><p>Equivalently: $$ \bar{X}_n \approx \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right) \quad \text{for large } n $$</p><p>This formulation of the CLT has profound implications for statistical estimation and inference:</p><ol><li><strong>Consistency</strong>: As sample size increases, the sample mean converges to the true population mean.</li><li><strong>Efficiency</strong>: The standard error of the mean decreases at a rate of $1/\sqrt{n}$.</li><li><strong>Normality</strong>: Regardless of the original distribution, the sampling distribution of the mean approaches normality.</li></ol><h2 id=assumptions-and-violations>Assumptions and Violations<a hidden class=anchor aria-hidden=true href=#assumptions-and-violations>#</a></h2><h3 id=key-assumptions>Key Assumptions<a hidden class=anchor aria-hidden=true href=#key-assumptions>#</a></h3><p>The classical CLT relies on several assumptions:</p><ol><li><strong>Independence</strong>: The random variables must be independent of each other.</li><li><strong>Identical distribution</strong>: The random variables must come from the same distribution.</li><li><strong>Finite variance</strong>: The variance $\sigma^2$ must be finite.</li></ol><h3 id=extensions-and-generalizations>Extensions and Generalizations<a hidden class=anchor aria-hidden=true href=#extensions-and-generalizations>#</a></h3><p>Several generalizations of the CLT exist for cases where the classical assumptions are not met:</p><ol><li><strong>Lyapunov CLT</strong>: Relaxes the identical distribution requirement, requiring only that no single variable dominates the sum.</li><li><strong>Lindeberg-Feller CLT</strong>: Provides more general conditions for the convergence of non-identical random variables.</li><li><strong>Martingale CLT</strong>: Extends to certain dependent sequences of random variables.</li><li><strong>Stable distributions</strong>: When the variance is infinite, sums may converge to non-normal stable distributions.</li></ol><h3 id=when-the-clt-fails>When the CLT Fails<a hidden class=anchor aria-hidden=true href=#when-the-clt-fails>#</a></h3><p>The CLT may not apply or may require larger sample sizes in several scenarios:</p><ol><li><strong>Heavy-tailed distributions</strong>: For distributions with infinite variance (e.g., Cauchy distribution), the CLT does not apply in its standard form.</li><li><strong>Strong dependencies</strong>: When variables are strongly correlated, the independence assumption is violated.</li><li><strong>Mixture distributions</strong>: For multimodal distributions, larger sample sizes may be needed for the CLT approximation to be accurate.</li><li><strong>Discrete distributions with few possible values</strong>: The convergence to normality may be slower.</li></ol><h2 id=applications-in-statistics>Applications in Statistics<a hidden class=anchor aria-hidden=true href=#applications-in-statistics>#</a></h2><p>The CLT underpins many statistical methods and applications:</p><h3 id=hypothesis-testing>Hypothesis Testing<a hidden class=anchor aria-hidden=true href=#hypothesis-testing>#</a></h3><p>Statistical tests like the t-test, z-test, and ANOVA rely on the CLT for their validity when sample sizes are sufficiently large.</p><h3 id=confidence-intervals>Confidence Intervals<a hidden class=anchor aria-hidden=true href=#confidence-intervals>#</a></h3><p>For large samples, we can construct confidence intervals for the population mean: $$ \bar{X} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}} $$</p><p>where $z_{\alpha/2}$ is the critical value from the standard normal distribution.</p><h3 id=quality-control>Quality Control<a hidden class=anchor aria-hidden=true href=#quality-control>#</a></h3><p>In manufacturing, the CLT justifies the use of control charts to monitor process means and detect deviations from target specifications.</p><h3 id=survey-sampling>Survey Sampling<a hidden class=anchor aria-hidden=true href=#survey-sampling>#</a></h3><p>The CLT provides the foundation for calculating margins of error in opinion polls and survey research.</p><h3 id=bootstrap-methods>Bootstrap Methods<a hidden class=anchor aria-hidden=true href=#bootstrap-methods>#</a></h3><p>The bootstrap technique relies on the CLT to approximate sampling distributions of statistics when analytical forms are unavailable.</p><h2 id=related-theorems>Related Theorems<a hidden class=anchor aria-hidden=true href=#related-theorems>#</a></h2><h3 id=law-of-large-numbers-lln>Law of Large Numbers (LLN)<a hidden class=anchor aria-hidden=true href=#law-of-large-numbers-lln>#</a></h3><p>The LLN states that the sample mean converges to the expected value as the sample size increases: $$ \bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \to \infty $$</p><p>where $\xrightarrow{p}$ denotes convergence in probability.</p><p>While the LLN tells us that the sample mean approaches the population mean, the CLT provides information about the distribution of that approximation and how quickly it converges.</p><h3 id=berry-esseen-theorem>Berry-Esseen Theorem<a hidden class=anchor aria-hidden=true href=#berry-esseen-theorem>#</a></h3><p>The Berry-Esseen theorem quantifies the rate of convergence to the normal distribution. For i.i.d. random variables with finite third moment $\rho = \mathbb{E}[|X-\mu|^3]$, the theorem provides an upper bound on the approximation error: $$ \sup_{x \in \mathbb{R}} |F_{Z_n}(x) - \Phi(x)| \leq \frac{C\rho}{\sigma^3\sqrt{n}} $$</p><p>where $F_{Z_n}$ is the CDF of $Z_n$, $\Phi$ is the standard normal CDF, and $C$ is a constant.</p><h3 id=multivariate-central-limit-theorem>Multivariate Central Limit Theorem<a hidden class=anchor aria-hidden=true href=#multivariate-central-limit-theorem>#</a></h3><p>The multivariate CLT extends the result to vector-valued random variables, stating that the normalized sum of i.i.d. random vectors converges to a multivariate normal distribution.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The Central Limit Theorem stands as one of the most profound results in probability theory, serving as a bridge between theoretical mathematics and practical statistics. Its assertion that sums of random variables tend toward normality, regardless of their original distribution, provides the foundation for numerous statistical methods and applications.</p><p>The universality of the CLT helps explain why the normal distribution appears so frequently in nature and human affairs—many observed phenomena represent the cumulative effect of numerous small, independent factors. From measurement errors to biological traits, financial returns to production quality, the CLT offers a mathematical explanation for the prevalence of bell-shaped distributions.</p><p>Understanding the CLT, including its assumptions, limitations, and generalizations, is essential for anyone working with data and statistical analysis. While the formal proof may involve sophisticated mathematics, the intuitive concept—that aggregating many random influences tends to produce a normal distribution—remains accessible and profoundly useful across disciplines.</p><p>As we collect larger datasets and develop more sophisticated analytical techniques, the Central Limit Theorem continues to serve as a cornerstone of statistical inference, allowing us to make reliable statements about populations based on samples, even when the underlying distributions are unknown or complex.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>DasGupta, A. (2008). <em>Asymptotic Theory of Statistics and Probability</em>. Springer.</li><li>Feller, W. (1971). <em>An Introduction to Probability Theory and Its Applications, Vol. 2</em>. Wiley.</li><li>Gnedenko, B.V., & Kolmogorov, A.N. (1968). <em>Limit Distributions for Sums of Independent Random Variables</em>. Addison-Wesley.</li><li>Le Cam, L. (1986). <em>Asymptotic Methods in Statistical Decision Theory</em>. Springer.</li><li>Lehmann, E.L. (1999). <em>Elements of Large-Sample Theory</em>. Springer.</li></ol><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://lcs-developer.com/tags/probability/>Probability</a></li><li><a href=https://lcs-developer.com/tags/math/>Math</a></li></ul><nav class=paginav><a class=next href=https://lcs-developer.com/posts/memory-pagination---a-comprehensive-guide/><span class=title>Next »</span><br><span>Memory Pagination - A Comprehensive Guide</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share The Central Limit Theorem on x" href="https://x.com/intent/tweet/?text=The%20Central%20Limit%20Theorem&amp;url=https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f&amp;hashtags=Probability%2cMath"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Central Limit Theorem on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f&amp;title=The%20Central%20Limit%20Theorem&amp;summary=The%20Central%20Limit%20Theorem&amp;source=https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Central Limit Theorem on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f&title=The%20Central%20Limit%20Theorem"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Central Limit Theorem on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Central Limit Theorem on whatsapp" href="https://api.whatsapp.com/send?text=The%20Central%20Limit%20Theorem%20-%20https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Central Limit Theorem on telegram" href="https://telegram.me/share/url?text=The%20Central%20Limit%20Theorem&amp;url=https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share The Central Limit Theorem on ycombinator" href="https://news.ycombinator.com/submitlink?t=The%20Central%20Limit%20Theorem&u=https%3a%2f%2flcs-developer.com%2fposts%2fthe-central-limit-theorem%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://lcs-developer.com/>LCS.Developer</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/XtremeXSPC/ rel=noopener target=_blank>Made with ❤️ by XtremeXSPC</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>